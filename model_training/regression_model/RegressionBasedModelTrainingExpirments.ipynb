{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HhHFolmi2x1",
        "outputId": "fd68abc5-7152-4123-fc4b-4b0133e7cfd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2025.11.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.6\n"
          ]
        }
      ],
      "source": [
        "!pip install evaluate\n",
        "!pip install datasets polars scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from datasets import DatasetDict, Dataset, load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    pipeline,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "from huggingface_hub import notebook_login\n",
        "from peft import LoraConfig, TaskType, get_peft_model\n",
        "import evaluate\n",
        "import matplotlib.pyplot as plt\n",
        "import wandb\n"
      ],
      "metadata": {
        "id": "cbbQ4YHPjCxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preperation"
      ],
      "metadata": {
        "id": "LipkM3zIOKfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed_value=6893):\n",
        "    \"\"\"Set seeds for reproducibility.\"\"\"\n",
        "    import random\n",
        "    import numpy as np\n",
        "    import torch\n",
        "\n",
        "    random.seed(seed_value)\n",
        "\n",
        "    np.random.seed(seed_value)\n",
        "\n",
        "    torch.manual_seed(seed_value)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    print(f\"Global seed set to {seed_value}\")\n",
        "\n",
        "set_seed()"
      ],
      "metadata": {
        "id": "nKM6IbkejTLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\n",
        "    \"siddharthgowda/articles_with_11day_ohlc_local\",\n",
        "    split=\"train\"\n",
        ")\n",
        "\n",
        "df_og = pl.from_arrow(dataset.data.table)\n",
        "\n",
        "df = df_og\n",
        "\n",
        "print(\"‚úÖ Successfully loaded data from public Hugging Face dataset.\")\n",
        "print(df.head(), df_og.shape)"
      ],
      "metadata": {
        "id": "-Lz2EPxejbZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df_og.with_columns(\n",
        "    # Calculate the Increase\n",
        "    log_return = (pl.col(\"Close_1\") / pl.col(\"Close_0\")).log()\n",
        ").with_columns(\n",
        "    target = pl.col(\"log_return\")\n",
        ")\n",
        "\n",
        "print(\"\\nüìù DataFrame with new Increase Target Label:\")\n",
        "print(df.select([\n",
        "    \"Stock_symbol\",\n",
        "    \"Article_title\",\n",
        "    \"Close_0\",\n",
        "    \"Close_1\",\n",
        "    \"log_return\",\n",
        "    \"target\"\n",
        "]).tail())"
      ],
      "metadata": {
        "id": "c9idejCjQtEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "LARGE_CAP_TICKERS = [\n",
        "    \"AAPL\", \"MSFT\", \"GOOGL\", \"GOOG\", \"AMZN\", \"NVDA\", \"META\", \"TSLA\", \"BRK.B\",\n",
        "    \"JPM\", \"V\", \"LLY\", \"XOM\", \"UNH\", \"JNJ\", \"WMT\", \"HD\", \"PG\", \"MA\", \"CVX\",\n",
        "    \"BAC\", \"COST\", \"PFE\", \"ABBV\", \"KO\", \"AVGO\", \"CSCO\", \"PEP\", \"MRK\", \"TMO\",\n",
        "    \"CRM\", \"CMCSA\", \"DIS\", \"PM\", \"ADBE\", \"ACN\", \"NKE\", \"INTU\", \"QCOM\", \"MCD\",\n",
        "    \"UPS\", \"ORCL\", \"GILD\", \"SBUX\", \"TXN\", \"AMGN\", \"LOW\", \"CAT\", \"MS\", \"AXP\",\n",
        "    \"BA\", \"GE\", \"LMT\", \"DE\", \"HON\", \"MMM\", \"GS\", \"SCHW\", \"CVS\", \"RTX\", \"MDLZ\",\n",
        "    \"FDX\", \"USB\", \"C\", \"MDT\", \"WFC\", \"BKNG\", \"TGT\", \"ISRG\", \"BIIB\", \"BMY\",\n",
        "    \"COP\", \"SLB\", \"EOG\", \"OXY\", \"KMI\", \"DHR\", \"EMR\", \"PPL\", \"SRE\", \"AEP\",\n",
        "    \"SO\", \"PCAR\", \"DTE\", \"EXC\", \"DUK\", \"NEE\", \"PEG\", \"AFL\", \"CB\", \"MMC\",\n",
        "    \"SPG\", \"AMT\", \"PLD\", \"EQIX\", \"ABT\", \"AT&T\", \"VZ\", \"TMUS\", \"LVS\", \"HCA\"\n",
        "]\n",
        "\n",
        "\n",
        "df_filtered = df.filter(pl.col(\"Stock_symbol\").is_in(LARGE_CAP_TICKERS))\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Original total rows in DataFrame: {len(df):,}\")\n",
        "print(f\"Number of large-cap tickers in filter list: {len(LARGE_CAP_TICKERS)}\")\n",
        "print(f\"Filtered rows (Non-Large-Cap Stocks only): {len(df_filtered):,}\")\n",
        "\n",
        "print(\"\\nSample of Filtered Data:\")\n",
        "print(df_filtered.head(5))\n",
        "\n",
        "present_tickers = df_filtered[\"Stock_symbol\"].unique().sort()\n",
        "print(f\"\\nUnique Non-Large-Cap Tickers found in your dataset: {len(present_tickers)}\")\n",
        "print(present_tickers.to_list()[:10], '...')\n"
      ],
      "metadata": {
        "id": "ypDkUBqBTnC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "RANDOM_SEED = 6893\n",
        "\n",
        "\n",
        "original_filtered_rows = df_filtered.height\n",
        "print(f\"Starting with full data size: {original_filtered_rows:,} rows\")\n",
        "\n",
        "df_pd = df_filtered.to_pandas()\n",
        "X = df_pd.drop(columns=[\"target\"])\n",
        "y = df_pd[\"target\"]\n",
        "\n",
        "# Perform 70/15/15 Train/Validation/Test Split\n",
        "TEST_SIZE = 0.15 # 15% for the final test set\n",
        "\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=TEST_SIZE,\n",
        "    shuffle=True,\n",
        "    random_state=RANDOM_SEED\n",
        ")\n",
        "\n",
        "\n",
        "VAL_SIZE_RELATIVE = 0.15 / (1.0 - TEST_SIZE)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp,\n",
        "    y_temp,\n",
        "    test_size=VAL_SIZE_RELATIVE,\n",
        "    shuffle=True,\n",
        "    random_state=RANDOM_SEED\n",
        ")\n",
        "\n",
        "df_train = pl.from_pandas(pd.concat([X_train, y_train], axis=1))\n",
        "df_val = pl.from_pandas(pd.concat([X_val, y_val], axis=1))\n",
        "df_test = pl.from_pandas(pd.concat([X_test, y_test], axis=1))\n",
        "\n",
        "\n",
        "print(\"\\n‚úÖ Split Verification:\")\n",
        "print(f\"Train Set Size: {df_train.height:,} rows (Actual: {df_train.height/original_filtered_rows:.2%})\")\n",
        "print(f\"Validation Set Size: {df_val.height:,} rows (Actual: {df_val.height/original_filtered_rows:.2%})\")\n",
        "print(f\"Test Set Size: {df_test.height:,} rows (Actual: {df_test.height/original_filtered_rows:.2%})\")\n",
        "\n",
        "print(\"\\nDistribution of 'target' in each set (describe for regression):\")\n",
        "print(\"--- Training Set ---\")\n",
        "print(df_train['target'].describe())\n",
        "\n",
        "print(\"--- Validation Set ---\")\n",
        "print(df_val['target'].describe())\n",
        "\n",
        "print(\"--- Test Set ---\")\n",
        "print(df_test['target'].describe())"
      ],
      "metadata": {
        "id": "HGZhiNwQRgfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model_name = \"roBERTa-base\""
      ],
      "metadata": {
        "id": "TDvDqtLNkmD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "\n",
        "def tokenize_fn(examples):\n",
        "    return tokenizer(\n",
        "        examples['Article_title'],\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "\n",
        "train_dataset = Dataset.from_pandas(df_train[['Article_title', 'target']].to_pandas().rename(columns={'target': 'labels'}))\n",
        "\n",
        "tokenized_dataset = train_dataset.map(tokenize_fn, batched=True)\n",
        "\n",
        "tokenized_dataset = tokenized_dataset.remove_columns(['Article_title'])\n",
        "\n",
        "print(tokenized_dataset[0])"
      ],
      "metadata": {
        "id": "CZhrWoOMkm4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_val_pd = df_val[['Article_title', 'target']].to_pandas()\n",
        "val_dataset = Dataset.from_pandas(df_val_pd.rename(columns={'target': 'labels'}))\n",
        "tokenized_val_dataset = val_dataset.map(tokenize_fn, batched=True)\n",
        "tokenized_val_dataset = tokenized_val_dataset.remove_columns(['Article_title'])"
      ],
      "metadata": {
        "id": "2q7Y1SpNfCH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_test_pd = df_test[['Article_title', 'target']].to_pandas()\n",
        "test_dataset = Dataset.from_pandas(df_test_pd.rename(columns={'target': 'labels'}))\n",
        "tokenized_test_dataset = test_dataset.map(tokenize_fn, batched=True)\n",
        "tokenized_test_dataset = tokenized_test_dataset.remove_columns(['Article_title'])"
      ],
      "metadata": {
        "id": "E9dX857d8PPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Head only Finetuning!"
      ],
      "metadata": {
        "id": "jDFVh5ppXEJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "id": "v_SOkvVWwCa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "id": "SkX460RGwEaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model_to_classify = AutoModelForSequenceClassification.from_pretrained(\n",
        "    base_model_name,\n",
        "    num_labels=1, # only 1 bc of regression\n",
        "    problem_type=\"regression\"\n",
        ")"
      ],
      "metadata": {
        "id": "GlGLi3KQm7dJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "\n",
        "    # Count parameters in each component\n",
        "    encoder_total = sum(p.numel() for p in model.roberta.parameters())\n",
        "    encoder_trainable = sum(p.numel() for p in model.roberta.parameters() if p.requires_grad)\n",
        "\n",
        "    head_total = sum(p.numel() for p in model.classifier.parameters())\n",
        "    head_trainable = sum(p.numel() for p in model.classifier.parameters() if p.requires_grad)\n",
        "\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    print(f\"{'Component':<20} {'Trainable':<15} {'Total':<15} {'Status'}\")\n",
        "    print(\"-\" * 65)\n",
        "    print(f\"{'Encoder (roberta)':<20} {encoder_trainable:<15,} {encoder_total:<15,} {'üîì Trainable' if encoder_trainable > 0 else 'üîí Frozen'}\")\n",
        "    print(f\"{'Head (classifier)':<20} {head_trainable:<15,} {head_total:<15,} {'üîì Trainable' if head_trainable > 0 else 'üîí Frozen'}\")\n",
        "    print(\"-\" * 65)\n",
        "    print(f\"{'TOTAL':<20} {trainable:<15,} {total:<15,} {f'{trainable/total*100:.1f}% trainable'}\")"
      ],
      "metadata": {
        "id": "eBS9v6ESmEc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model_to_classify.base_model.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "# unfreezing last encoder layer\n",
        "last_layer_prefix = 'roberta.encoder.layer.11.'\n",
        "for name, param in model_to_classify.named_parameters():\n",
        "    if name.startswith(last_layer_prefix):\n",
        "        param.requires_grad = True\n",
        "# unfreezing classification head\n",
        "for param in model_to_classify.classifier.parameters():\n",
        "    param.requires_grad = True"
      ],
      "metadata": {
        "id": "F-pHo1liW681"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_trainable_parameters(model_to_classify)"
      ],
      "metadata": {
        "id": "MpeEOLMVd3l8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WandB Setup\n",
        "os.environ[\"WANDB_PROJECT\"] = \"stock-prediction-fine-tuning-project-regression-v2-head-normalized-no-large-stocks-tfid\"\n",
        "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./headResults\",\n",
        "    num_train_epochs=2,\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=128,\n",
        "    warmup_steps=0,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./headLogs\",\n",
        "    logging_steps=10,\n",
        "    report_to=\"wandb\",\n",
        "    save_strategy=\"epoch\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    logging_strategy=\"steps\",\n",
        "    max_grad_norm=1.0,\n",
        "    # Mixed Precision Training for A100 speedup\n",
        "    fp16=True,\n",
        "    warmup_ratio=0.10,\n",
        "    # 3. Model Selection\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model_to_classify,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    eval_dataset=tokenized_val_dataset, # Use the tokenized validation dataset\n",
        "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer)\n",
        ")"
      ],
      "metadata": {
        "id": "v9Ts2Wtlah7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Training will run for {training_args.num_train_epochs} epochs.\")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "wZicVRA-ajby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Second Round of Finetuning"
      ],
      "metadata": {
        "id": "K00fbzn6YDs3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "id": "k9njWoCq-IYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model_to_classify.parameters():\n",
        "  param.requires_grad = True\n",
        "\n",
        "for param in model_to_classify.classifier.parameters():\n",
        "  param.requires_grad = True\n",
        "\n",
        "print_trainable_parameters(model_to_classify)"
      ],
      "metadata": {
        "id": "CqDzOe8Fl_7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"WANDB_PROJECT\"] = \"stock-prediction-fine-tuning-project-regression-v1-full-normalized-no-large-stocks-tfid\"\n",
        "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./fullModelResults\",\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=128,\n",
        "    warmup_steps=0,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./fullModelLogs\",\n",
        "    logging_steps=10,\n",
        "    report_to=\"wandb\",\n",
        "    save_strategy=\"epoch\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    logging_strategy=\"steps\",\n",
        "    max_grad_norm=1.0,\n",
        "    fp16=True,\n",
        "    warmup_ratio=0.06,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model_to_classify,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    eval_dataset=tokenized_val_dataset,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer)\n",
        ")"
      ],
      "metadata": {
        "id": "KWFDm5vDcOAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Training will run for {training_args.num_train_epochs} epochs.\")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "dhn8nVfbhoO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "id": "XC9_jdjWDVx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "vdYseX3IbsnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training set\n",
        "# refetch model from wandb\n",
        "wandb.init(project=\"stock-prediction-fine-tuning-project-regression-v1-full-normalized-no-large-stocks-tfid\", job_type=\"evaluation_train_set\")\n",
        "\n",
        "print(\"\\n--- Evaluating on Training Dataset ---\")\n",
        "prediction_output_train = trainer.predict(test_dataset=tokenized_dataset)\n",
        "predictions_train = prediction_output_train.predictions.flatten()\n",
        "true_labels_train = prediction_output_train.label_ids\n",
        "\n",
        "mse_train = mean_squared_error(true_labels_train, predictions_train)\n",
        "r2_train = r2_score(true_labels_train, predictions_train)\n",
        "\n",
        "print(f\"Mean Squared Error (MSE) on Training Set: {mse_train:.4f}\")\n",
        "print(f\"R-squared (R2) on Training Set: {r2_train:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(true_labels_train, predictions_train, alpha=0.3)\n",
        "plt.plot([min(true_labels_train), max(true_labels_train)], [min(true_labels_train), max(true_labels_train)], color='red', linestyle='--', label='Perfect Prediction')\n",
        "plt.title('True Log Return vs. Predicted Log Return (Training Set)')\n",
        "plt.xlabel('True Log Return')\n",
        "plt.ylabel('Predicted Log Return')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "gbf3DwNQqyrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test set\n",
        "wandb.init(project=\"stock-prediction-fine-tuning-project-regression-v1-full-normalized-no-large-stocks-tfid\", job_type=\"evaluation_test_set\")\n",
        "\n",
        "print(\"\\n--- Evaluating on Test Dataset ---\")\n",
        "prediction_output_test = trainer.predict(test_dataset=tokenized_test_dataset)\n",
        "predictions_test = prediction_output_test.predictions.flatten()\n",
        "true_labels_test = prediction_output_test.label_ids\n",
        "\n",
        "mse_test = mean_squared_error(true_labels_test, predictions_test)\n",
        "r2_test = r2_score(true_labels_test, predictions_test)\n",
        "\n",
        "print(f\"Mean Squared Error (MSE) on Test Set: {mse_test:.4f}\")\n",
        "print(f\"R-squared (R2) on Test Set: {r2_test:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(true_labels_test, predictions_test, alpha=0.3)\n",
        "plt.plot([min(true_labels_test), max(true_labels_test)], [min(true_labels_test), max(true_labels_test)], color='red', linestyle='--', label='Perfect Prediction')\n",
        "plt.title('True Log Return vs. Predicted Log Return (Test Set)')\n",
        "plt.xlabel('True Log Return')\n",
        "plt.ylabel('Predicted Log Return')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "tImzq6MLpeSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_login()"
      ],
      "metadata": {
        "id": "gI6Zur1K6Zdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_repo_name = \"siddharthgowda/roberta-stock-news-regression-prediction\"\n",
        "trainer.push_to_hub(model_repo_name)\n",
        "tokenizer.push_to_hub(model_repo_name)"
      ],
      "metadata": {
        "id": "anaRDJiN6LZ-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}